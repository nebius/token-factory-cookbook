{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAL MCP Integration with Nebius Token Factory\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nebius/token-factory-cookbook/blob/main/agents/pal-mcp-integration/pal_nebius_tutorial.ipynb)\n",
    "[![](https://img.shields.io/badge/Powered%20by-Nebius-orange?style=flat&labelColor=darkblue&color=orange)](http://tokenfactory.nebius.com/)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to integrate **Nebius Token Factory** with **PAL MCP Server** to orchestrate multiple AI models through Claude Code.\n",
    "\n",
    "**PAL MCP** (Provider Abstraction Layer - Model Context Protocol) enables:\n",
    "- Multi-model orchestration from Claude Code\n",
    "- Conversation continuity across different AI models\n",
    "- Specialized tools for code review, debugging, consensus\n",
    "\n",
    "**Nebius Token Factory** provides:\n",
    "- Access to top open-source models (Qwen3, DeepSeek, Llama, GLM, GPT-OSS)\n",
    "- OpenAI-compatible API\n",
    "- Extended context windows (up to 262K tokens)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Claude Code CLI** installed ([claude.ai/code](https://claude.ai/code))\n",
    "- **Nebius API Key** from [tokenfactory.nebius.com](https://tokenfactory.nebius.com/)\n",
    "- **Python 3.10+** and **uv** package manager\n",
    "\n",
    "## References\n",
    "\n",
    "- [PAL MCP Server GitHub](https://github.com/BeehiveInnovations/pal-mcp-server)\n",
    "- [Nebius Token Factory Documentation](https://docs.tokenfactory.nebius.com/)\n",
    "- [Claude Code Documentation](https://claude.ai/code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Getting Started\n",
    "\n",
    "### 1.1 - Get Your Nebius API Key\n",
    "\n",
    "1. Visit [Nebius Token Factory](https://tokenfactory.nebius.com/)\n",
    "2. Sign up for a free account\n",
    "3. Navigate to API Keys section\n",
    "4. Copy your API key\n",
    "\n",
    "### 1.2 - If Running on Google Colab\n",
    "\n",
    "Add `NEBIUS_API_KEY` to **Secrets** panel:\n",
    "\n",
    "![Colab Secrets](https://github.com/nebius/token-factory-cookbook/raw/main/images/google-colab-1.png)\n",
    "\n",
    "### 1.3 - If Running Locally\n",
    "\n",
    "We'll configure the PAL MCP Server `.env` file during installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Install Dependencies\n",
    "\n",
    "We'll install the OpenAI SDK to demonstrate direct API calls and compare with PAL MCP usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Recommended way of getting configuration\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    print(\"Running in Colab\")\n",
    "    from google.colab import userdata\n",
    "    NEBIUS_API_KEY = userdata.get('NEBIUS_API_KEY')\n",
    "else:\n",
    "    print(\"NOT running in Colab\")\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    NEBIUS_API_KEY = os.getenv('NEBIUS_API_KEY')\n",
    "\n",
    "# Quick hack (not recommended) - hardcode key\n",
    "# NEBIUS_API_KEY = \"your_key_here\"\n",
    "\n",
    "if NEBIUS_API_KEY:\n",
    "    print('‚úÖ NEBIUS_API_KEY found')\n",
    "    os.environ['NEBIUS_API_KEY'] = NEBIUS_API_KEY\n",
    "else:\n",
    "    raise RuntimeError('‚ùå NEBIUS_API_KEY NOT found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Understand Nebius Models Available via PAL MCP\n",
    "\n",
    "PAL MCP provides access to Nebius Token Factory models with convenient aliases.\n",
    "\n",
    "### Key Models\n",
    "\n",
    "| Model | Alias | Context | Best For |\n",
    "|-------|-------|---------|----------|\n",
    "| **Qwen3-235B-Instruct** | `nebius-qwen3` | 262K | General reasoning, coding |\n",
    "| **Qwen3-235B-Thinking** | `nebius-qwen3-thinking` | 262K | Extended reasoning |\n",
    "| **GPT-OSS-120B** | `nebius-gpt-oss` | 128K | OpenAI-style reasoning |\n",
    "| **DeepSeek-R1** | `nebius-deepseek-r1` | 128K | Chain-of-thought reasoning |\n",
    "| **DeepSeek-V3.2** | `nebius-deepseek` | 128K | Fast general purpose |\n",
    "| **Llama-3.3-70B** | `nebius-llama` | 128K | Balanced performance |\n",
    "| **GLM-4.5** | `nebius-glm` | 128K | Vision + function calling |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Test Direct Nebius API Access\n",
    "\n",
    "Before setting up PAL MCP, let's verify direct API access works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 - Initialize OpenAI Client for Nebius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Create client pointing to Nebius Token Factory\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.environ.get('NEBIUS_API_KEY')\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Nebius client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Test Basic Model Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Test with Qwen3-235B\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen3-235B-A22B-Instruct-2507\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain what Nebius Token Factory is in one sentence.\"}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"\\nüìù Model Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\nüìä Token Usage:\", response.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 - Test Multiple Models\n",
    "\n",
    "Let's test different Nebius models to see their capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "models_to_test = [\n",
    "    (\"Qwen/Qwen3-235B-A22B-Instruct-2507\", \"Qwen3 235B\"),\n",
    "    (\"openai/gpt-oss-120b\", \"GPT-OSS 120B\"),\n",
    "    (\"deepseek-ai/DeepSeek-V3.2\", \"DeepSeek V3.2\"),\n",
    "    (\"meta-llama/Llama-3.3-70B-Instruct\", \"Llama 3.3 70B\")\n",
    "]\n",
    "\n",
    "question = \"What is 7 * 8 + 15?\"\n",
    "\n",
    "print(f\"üßÆ Testing question: {question}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_id, model_name in models_to_test:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": question}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        print(f\"\\nü§ñ {model_name}:\")\n",
    "        print(f\"   {answer[:150]}...\" if len(answer) > 150 else f\"   {answer}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå {model_name}: Error - {str(e)[:100]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Install and Configure PAL MCP Server\n",
    "\n",
    "Now let's set up PAL MCP to orchestrate these models through Claude Code.\n",
    "\n",
    "### 6.1 - Installation Steps\n",
    "\n",
    "**Note**: These commands should be run in your terminal, not in this notebook.\n",
    "\n",
    "```bash\n",
    "# Clone PAL MCP Server\n",
    "cd ~/Desktop  # or your preferred location\n",
    "git clone https://github.com/BeehiveInnovations/pal-mcp-server.git\n",
    "cd pal-mcp-server\n",
    "\n",
    "# Run automatic setup\n",
    "./run-server.sh\n",
    "```\n",
    "\n",
    "The setup script will:\n",
    "1. Create Python virtual environment\n",
    "2. Install dependencies\n",
    "3. Prompt for API keys\n",
    "4. Configure Claude Code integration\n",
    "5. Test the connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Configure Nebius Integration\n",
    "\n",
    "Edit `~/Desktop/pal-mcp-server/.env` to add your Nebius configuration:\n",
    "\n",
    "```bash\n",
    "# Nebius Token Factory API Key\n",
    "NEBIUS_API_KEY=your_api_key_here\n",
    "\n",
    "# Optional: Restrict available models (leave empty for all)\n",
    "# NEBIUS_ALLOWED_MODELS=nebius-qwen3,nebius-deepseek-r1,nebius-gpt-oss\n",
    "\n",
    "# Model Selection\n",
    "DEFAULT_MODEL=auto  # Let Claude choose best model\n",
    "\n",
    "# Tool Configuration (enable essential tools, disable heavy ones)\n",
    "DISABLED_TOOLS=analyze,refactor,testgen,secaudit,docgen,tracer\n",
    "\n",
    "# Conversation Settings\n",
    "CONVERSATION_TIMEOUT_HOURS=24\n",
    "MAX_CONVERSATION_TURNS=40\n",
    "\n",
    "# Logging\n",
    "LOG_LEVEL=INFO\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - Verify Claude Code Configuration\n",
    "\n",
    "Check that Claude Code is configured to use PAL MCP:\n",
    "\n",
    "```bash\n",
    "# View Claude Code settings\n",
    "cat ~/.claude/settings.json\n",
    "```\n",
    "\n",
    "You should see PAL configured under `mcpServers`:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"pal\": {\n",
    "      \"command\": \"/path/to/pal-mcp-server/.pal_venv/bin/python\",\n",
    "      \"args\": [\"/path/to/pal-mcp-server/server.py\"],\n",
    "      \"env\": {\n",
    "        \"NEBIUS_API_KEY\": \"your_key\",\n",
    "        \"DEFAULT_MODEL\": \"auto\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7 - Using PAL MCP with Claude Code\n\nNow that PAL MCP is installed, let's explore how to use it from Claude Code.\n\nPAL tools can be invoked two ways:\n1. **Slash commands** (direct): `/pal:chat`, `/pal:consensus`, `/pal:codereview`, etc.\n2. **Natural language** (indirect): Claude interprets your request and calls the appropriate PAL tool\n\n### 7.1 - List Available Models\n\n```\n/pal:listmodels\n```\n\nClaude will show all available Nebius models with their aliases and capabilities.\n\n### 7.2 - Simple Chat with a Nebius Model\n\n```\n/pal:chat with nebius-qwen3 explain what a Python decorator is\n```\n\nOr with a specific prompt:\n\n```\n/pal:chat model=nebius-deepseek-r1 What are the tradeoffs between Redis and Memcached for session storage?\n```\n\n### 7.3 - Extended Reasoning\n\n```\n/pal:thinkdeep with nebius-deepseek-r1 analyze the time complexity of merge sort vs timsort\n```\n\nDeepSeek-R1 provides visible chain-of-thought reasoning with its analysis.\n\n### 7.4 - Quick Planning\n\n```\n/pal:planner with nebius-gpt-oss break down the migration from REST to GraphQL for our user service\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8 - Multi-Model Consensus Workflows\n\nOne of PAL MCP's most powerful features is multi-model consensus ‚Äî getting multiple Nebius models to weigh in on a decision.\n\n### 8.1 - Architecture Decision\n\n```\n/pal:consensus with nebius-qwen3, nebius-deepseek-r1, and nebius-gpt-oss Should we use REST API or GraphQL for our new microservice?\n```\n\n**What happens:**\n1. Claude sends the question to all three models\n2. Each model provides its perspective:\n   - **Qwen3**: General reasoning with broad coverage\n   - **DeepSeek-R1**: Deep chain-of-thought analysis\n   - **GPT-OSS**: OpenAI-style balanced assessment\n3. Claude synthesizes a final recommendation with agreement/disagreement summary\n\n### 8.2 - Code Design Consensus\n\n```\n/pal:consensus with nebius-qwen3 and nebius-llama Should we use inheritance or composition for this class hierarchy?\n```\n\n### 8.3 - Algorithm Selection\n\n```\n/pal:consensus with nebius-deepseek-r1, nebius-gpt-oss, and nebius-qwen3-thinking Choose the best sorting algorithm for: 10M records, 2GB memory limit, partially sorted data\n```\n\n### 8.4 - Technology Stack Decision\n\n```\n/pal:consensus with nebius-qwen3, nebius-deepseek, and nebius-gpt-oss For a real-time chat app, should we use WebSockets, SSE, or long polling?\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9 - Code Review Workflows\n\nPAL MCP's `codereview` tool enables comprehensive multi-model code reviews.\n\n### 9.1 - Single Model Review\n\n```\n/pal:codereview with nebius-qwen3 Review the auth module for security issues and code quality\n```\n\nClaude will:\n- Walk through code systematically\n- Track confidence levels (exploring ‚Üí low ‚Üí medium ‚Üí high ‚Üí certain)\n- Identify issues by severity (critical ‚Üí high ‚Üí medium ‚Üí low)\n- Provide actionable feedback\n\n### 9.2 - Multi-Model Collaborative Review\n\n```\n/pal:codereview with nebius-qwen3 and nebius-deepseek-r1 Review src/api/ for security vulnerabilities and race conditions\n```\n\nThen follow up with a plan:\n\n```\n/pal:planner with nebius-gpt-oss Based on the previous code review findings, create a fix strategy\n```\n\n**Workflow:**\n1. **Qwen3**: Initial comprehensive analysis\n2. **DeepSeek-R1**: Deep reasoning catches subtle issues\n3. **Claude**: Consolidates findings from both\n4. **GPT-OSS**: Creates prioritized implementation plan\n\n### 9.3 - Pre-Commit Validation\n\nAfter making changes, validate before committing:\n\n```\n/pal:precommit with nebius-qwen3 Validate my staged changes for regressions\n```\n\n### 9.4 - Challenge Your Own Code\n\n```\n/pal:challenge with nebius-gpt-oss Is my singleton pattern here actually necessary, or am I over-engineering?\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10 - Advanced Multi-Model Patterns\n\n### 10.1 - Thinking Model Chain\n\nCombine thinking models for maximum reasoning depth:\n\n```\n/pal:thinkdeep with nebius-deepseek-r1 Analyze this algorithm for correctness and edge cases\n```\n\nThen validate with a second thinking model:\n\n```\n/pal:chat with nebius-qwen3-thinking Validate the previous analysis ‚Äî are there cases it missed?\n```\n\n### 10.2 - Debugging with PAL\n\n```\n/pal:debug with nebius-deepseek-r1 Investigate this race condition in the payment processing module\n```\n\nDeepSeek-R1 performs systematic root cause analysis with confidence tracking.\n\n### 10.3 - API Documentation Lookup\n\nWhen you need current docs (not stale training data):\n\n```\n/pal:apilookup Get the latest FastAPI documentation for WebSocket support\n```\n\n### 10.4 - Vision Model Integration\n\nAnalyze diagrams or screenshots with vision-capable models:\n\n```\n/pal:chat with nebius-glm Analyze this architecture diagram and explain the data flow\n```\n\n```\n/pal:chat with nebius-qwen-vl What does this error screenshot show?\n```\n\n### 10.5 - CLI-to-CLI Bridging\n\nSpawn isolated sub-agents for context-heavy tasks:\n\n```\n/pal:clink with cli_name=\"gemini\" role=\"codereviewer\" Audit the auth module for security issues\n```\n\nThe sub-agent works in a fresh context and returns only the final report."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 - Example: Complete Code Review Workflow\n",
    "\n",
    "Let's walk through a complete example using Python code.\n",
    "\n",
    "### 11.1 - Sample Code to Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code with intentional issues\n",
    "sample_code = '''\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total = total + num\n",
    "    return total / len(numbers)\n",
    "\n",
    "def process_user_data(user_input):\n",
    "    # Execute user command directly\n",
    "    result = eval(user_input)\n",
    "    return result\n",
    "\n",
    "class DatabaseConnection:\n",
    "    def __init__(self, host, password):\n",
    "        self.host = host\n",
    "        self.password = password\n",
    "        print(f\"Connecting to {host} with password: {password}\")\n",
    "    \n",
    "    def execute(self, query):\n",
    "        # SQL injection vulnerable\n",
    "        full_query = f\"SELECT * FROM users WHERE name = '{query}'\"\n",
    "        return full_query\n",
    "'''\n",
    "\n",
    "print(\"üìù Sample Code for Review:\")\n",
    "print(sample_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 11.2 - Review Using Multiple Nebius Models\n\nIn Claude Code, run:\n\n```\n/pal:codereview with nebius-qwen3 and nebius-deepseek-r1 Review this code for security issues, bugs, and performance problems\n\n[paste the sample_code above]\n```\n\n**Expected Analysis:**\n\n**From Qwen3:**\n- **CRITICAL**: `eval()` usage in `process_user_data` ‚Äî arbitrary code execution\n- **CRITICAL**: SQL injection in `DatabaseConnection.execute()`\n- **HIGH**: Password logging in `DatabaseConnection.__init__`\n- **MEDIUM**: No error handling in `calculate_average` (division by zero)\n- **LOW**: Inefficient loop in `calculate_average` (use `sum()`)\n\n**From DeepSeek-R1 (with chain-of-thought reasoning):**\n- Analyzes attack vectors for `eval()` injection\n- Demonstrates SQL injection exploit scenario\n- Considers edge cases (empty list, None values)\n- Suggests parameterized queries and input validation\n\n**Then create a fix plan:**\n\n```\n/pal:planner with nebius-gpt-oss Based on the code review above, create a prioritized fix strategy\n```\n\nClaude combines both analyses into a single action plan."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 - Simulate Model Responses\n",
    "\n",
    "Let's simulate what each Nebius model might focus on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate reviews from different models\n",
    "reviews = {\n",
    "    \"Qwen3-235B\": {\n",
    "        \"focus\": \"Comprehensive coverage\",\n",
    "        \"findings\": [\n",
    "            \"CRITICAL: eval() allows arbitrary code execution\",\n",
    "            \"CRITICAL: SQL injection vulnerability\",\n",
    "            \"HIGH: Password exposed in logs\",\n",
    "            \"MEDIUM: No error handling for empty list\",\n",
    "            \"LOW: Use sum() instead of manual loop\"\n",
    "        ]\n",
    "    },\n",
    "    \"DeepSeek-R1\": {\n",
    "        \"focus\": \"Deep security reasoning\",\n",
    "        \"findings\": [\n",
    "            \"CRITICAL: eval() - attacker could execute: eval('__import__(\\\"os\\\").system(\\\"rm -rf /\\\")')\",\n",
    "            \"CRITICAL: SQL injection - input \\\"'; DROP TABLE users; --\\\" would be catastrophic\",\n",
    "            \"HIGH: Password in plaintext memory could be scraped via debugging\",\n",
    "            \"MEDIUM: calculate_average([]) causes ZeroDivisionError\",\n",
    "            \"Recommendation: Use ast.literal_eval() or JSON parsing instead of eval()\"\n",
    "        ]\n",
    "    },\n",
    "    \"GPT-OSS-120B\": {\n",
    "        \"focus\": \"Balanced security + architecture\",\n",
    "        \"findings\": [\n",
    "            \"CRITICAL: Replace eval() with safer alternatives (json.loads, ast.literal_eval)\",\n",
    "            \"CRITICAL: Use parameterized queries (e.g., cursor.execute(\\\"SELECT * WHERE name=%s\\\", (name,)))\",\n",
    "            \"HIGH: Use logging.debug() instead of print(), mask sensitive data\",\n",
    "            \"MEDIUM: Add input validation and empty list check\",\n",
    "            \"Architecture note: Consider using ORM (SQLAlchemy) to prevent SQL injection\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üîç Simulated Multi-Model Code Review\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model, review in reviews.items():\n",
    "    print(f\"\\nü§ñ {model}\")\n",
    "    print(f\"   Focus: {review['focus']}\\n\")\n",
    "    for finding in review['findings']:\n",
    "        print(f\"   ‚Ä¢ {finding}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüí° Consensus: All models agree on critical security issues\")\n",
    "print(\"   Priority: Fix eval() and SQL injection immediately\")\n",
    "print(\"   Next: Address password logging and error handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 - Model Selection Strategy\n",
    "\n",
    "### 12.1 - When to Use Each Model\n",
    "\n",
    "| Scenario | Recommended Model | Why |\n",
    "|----------|-------------------|-----|\n",
    "| **Quick questions** | `nebius-llama` | Fast, efficient, good for simple tasks |\n",
    "| **Code generation** | `nebius-qwen3` | Excellent coding capabilities, 262K context |\n",
    "| **Deep reasoning** | `nebius-deepseek-r1` | Chain-of-thought, visible reasoning |\n",
    "| **Extended thinking** | `nebius-qwen3-thinking` | Dedicated thinking mode |\n",
    "| **Security audit** | `nebius-deepseek-r1` | Strong at analyzing attack vectors |\n",
    "| **Architecture review** | `nebius-gpt-oss` | Balanced, OpenAI-style reasoning |\n",
    "| **Vision tasks** | `nebius-glm` or `nebius-qwen-vl` | Multimodal capabilities |\n",
    "| **Consensus** | Mix of 3 models | Diverse perspectives |\n",
    "\n",
    "### 12.2 - Cost Optimization\n",
    "\n",
    "- **Tier 1 (Expensive, High Intelligence)**: Qwen3-235B, GPT-OSS-120B\n",
    "- **Tier 2 (Moderate)**: DeepSeek-V3.2, Llama-3.3-70B\n",
    "- **Tier 3 (Cost-effective)**: Llama-3.1-8B, Gemma-27B\n",
    "\n",
    "**Strategy**: Start with Tier 2-3 for exploration, escalate to Tier 1 for production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 13 - Conversation Continuity Example\n\nOne of PAL's key features is maintaining context across models. Each tool call builds on the previous conversation thread.\n\n### Step-by-step in Claude Code:\n\n**Step 1**: Start a conversation with one model:\n```\n/pal:chat with nebius-qwen3 Explain microservices architecture patterns for e-commerce\n```\n\n**Step 2**: Continue with a different model (it sees Step 1's full context):\n```\n/pal:chat with nebius-deepseek-r1 Based on the previous explanation, what are the most common pitfalls?\n```\n\n**Step 3**: Get consensus (both models see the full thread):\n```\n/pal:consensus with nebius-gpt-oss and nebius-llama Given the discussion so far, what are the top 3 best practices?\n```\n\n**This is impossible with direct API calls** ‚Äî PAL maintains conversation state across models, so DeepSeek-R1 in Step 2 knows exactly what Qwen3 said in Step 1."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 14 - Debugging Workflow Example\n\n### 14.1 - Systematic Debug with PAL\n\n```\n/pal:debug with nebius-deepseek-r1 This function throws a race condition under high concurrency ‚Äî investigate root cause\n\n[paste error trace and relevant code]\n```\n\nDeepSeek-R1 performs:\n1. **Hypothesis generation** (confidence: exploring)\n2. **Code analysis** (confidence: low ‚Üí medium)\n3. **Root cause identification** (confidence: high)\n4. **Solution proposal** (confidence: certain)\n\n### 14.2 - Validate the Fix\n\n```\n/pal:chat with nebius-qwen3 Review this proposed fix for the race condition ‚Äî check correctness and edge cases\n\n[paste proposed solution]\n```\n\n### 14.3 - Pre-Commit Check\n\n```\n/pal:precommit with nebius-gpt-oss Validate that the race condition fix doesn't introduce regressions\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 15 - Best Practices Summary\n\n### ‚úÖ Do's\n\n1. **Use auto mode** for general tasks ‚Äî let PAL choose the best model\n2. **Leverage consensus** for important decisions (3 models minimum)\n3. **Combine thinking models** (DeepSeek-R1 + Qwen3-Thinking) for complex reasoning\n4. **Use codereview workflow** before major commits\n5. **Validate with precommit** to catch regressions\n6. **Start with smaller models** for exploration, scale up for production\n7. **Maintain conversation continuity** ‚Äî reference previous exchanges\n\n### ‚ùå Don'ts\n\n1. **Don't use expensive models** for simple tasks (use `nebius-llama-8b` or `nebius-gemma`)\n2. **Don't skip validation** ‚Äî always get a second opinion on critical code\n3. **Don't ignore consensus** ‚Äî if models disagree, investigate why\n4. **Don't use vision models** for text-only tasks (wastes resources)\n5. **Don't exceed context limits** ‚Äî break large files into chunks\n6. **Don't forget to configure** `NEBIUS_ALLOWED_MODELS` for cost control\n\n### üéØ Workflow Patterns (Slash Commands)\n\n**Pattern 1: Quick Task**\n```\n/pal:chat with nebius-llama What does this error mean?\n```\n\n**Pattern 2: Code Review**\n```\n/pal:codereview with nebius-qwen3 Review src/auth/ for issues\n/pal:precommit with nebius-qwen3 Validate my changes\n```\n\n**Pattern 3: Complex Decision**\n```\n/pal:consensus with nebius-qwen3, nebius-deepseek-r1, nebius-gpt-oss Should we use Kafka or RabbitMQ?\n/pal:thinkdeep with nebius-deepseek-r1 Dig deeper into the latency tradeoffs\n/pal:planner with nebius-gpt-oss Create an implementation plan based on the consensus\n```\n\n**Pattern 4: Full Development Cycle**\n```\n/pal:planner with nebius-qwen3 Design the new payment module\n/pal:chat with nebius-qwen3 Help implement the PaymentProcessor class\n/pal:codereview with nebius-qwen3 and nebius-deepseek-r1 Review the payment module\n/pal:debug with nebius-deepseek-r1 Investigate the timeout in processPayment()\n/pal:precommit with nebius-gpt-oss Final check before merge\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16 - Troubleshooting\n",
    "\n",
    "### Issue: PAL tools not available in Claude Code\n",
    "\n",
    "**Solution:**\n",
    "```bash\n",
    "# Check Claude Code settings\n",
    "cat ~/.claude/settings.json\n",
    "\n",
    "# Restart Claude Code\n",
    "# Test with: \"use pal to list models\"\n",
    "```\n",
    "\n",
    "### Issue: \"Model not found\" error\n",
    "\n",
    "**Solution:**\n",
    "```bash\n",
    "# Check allowed models in .env\n",
    "cat ~/Desktop/pal-mcp-server/.env | grep NEBIUS_ALLOWED_MODELS\n",
    "\n",
    "# Leave empty to enable all models\n",
    "NEBIUS_ALLOWED_MODELS=\n",
    "```\n",
    "\n",
    "### Issue: Timeout errors\n",
    "\n",
    "**Solution:**\n",
    "```bash\n",
    "# Increase timeouts in .env\n",
    "CUSTOM_READ_TIMEOUT=1800.0\n",
    "CUSTOM_WRITE_TIMEOUT=1800.0\n",
    "```\n",
    "\n",
    "### Issue: API key not working\n",
    "\n",
    "**Solution:**\n",
    "```bash\n",
    "# Verify API key format (no quotes, no spaces)\n",
    "cat ~/Desktop/pal-mcp-server/.env | grep NEBIUS_API_KEY\n",
    "\n",
    "# Test direct API access\n",
    "curl -H \"Authorization: Bearer $NEBIUS_API_KEY\" \\\n",
    "     https://api.studio.nebius.com/v1/models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17 - Next Steps\n",
    "\n",
    "### Explore More Features\n",
    "\n",
    "1. **Try other PAL tools:**\n",
    "   - `analyze` - Codebase architecture analysis\n",
    "   - `refactor` - Intelligent refactoring suggestions\n",
    "   - `testgen` - Test generation\n",
    "   - `secaudit` - Security audits\n",
    "\n",
    "2. **Experiment with model combinations:**\n",
    "   - Find your optimal 3-model consensus set\n",
    "   - Test different models for different task types\n",
    "\n",
    "3. **Build custom workflows:**\n",
    "   - Create project-specific review checklists\n",
    "   - Define team conventions for model selection\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [PAL MCP Documentation](https://github.com/BeehiveInnovations/pal-mcp-server/blob/main/docs/index.md)\n",
    "- [Nebius Model Catalog](https://tokenfactory.nebius.com/)\n",
    "- [Claude Code Guide](https://claude.ai/code)\n",
    "- [Token Factory Cookbook](https://github.com/nebius/token-factory-cookbook)\n",
    "\n",
    "### Community\n",
    "\n",
    "- [PAL MCP Issues](https://github.com/BeehiveInnovations/pal-mcp-server/issues)\n",
    "- [Token Factory Discord](https://discord.gg/nebius)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy multi-model orchestration! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}