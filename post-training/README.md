# Post-Training Examples

This directory contains practical examples and tutorials for post-training techniques using Nebius Token Factory. Post-training allows you to customize and improve pre-trained language models for your specific use cases through fine-tuning and other optimization methods.

## Overview

Post-training encompasses techniques to adapt foundation models to specific domains, tasks, or behaviors without training from scratch. This repository demonstrates how to leverage Nebius Token Factory's infrastructure and APIs to perform efficient post-training workflows.

## Documentation

For comprehensive guides and API references, see the official [Post-Training Documentation](https://docs.tokenfactory.nebius.com/post-training/overview).

## Example 1: Fine-Tuning Meta-Llama-3.1

Demonstrates fine-tuning the Meta-Llama-3.1 model using Nebius Token Factory. This example walks through dataset preparation, training configuration, and model evaluation.

**[View Example â†’](fine-tuning-1/)**



## Additional Resources

- [Nebius Token Factory Documentation](https://docs.tokenfactory.nebius.com/)
- [API Reference](https://docs.tokenfactory.nebius.com/api/)

## Contributing

Found an issue or have suggestions for improvements? Contributions and feedback are welcome!