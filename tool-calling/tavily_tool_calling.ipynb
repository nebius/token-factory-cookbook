{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tavily Search as a Tool in Nebius Token Factory\n",
        "\n",
        "[![](https://img.shields.io/badge/Powered%20by-Nebius%20AI-orange?style=flat&labelColor=darkblue&color=green)](http://tokenfactory.nebius.com/)\n",
        "\n",
        "This notebook demonstrates how to use [Tavily Search](https://tavily.com/) as a tool with function calling on the Nebius Token Factory API. The LLM decides when to search the web and processes the results to answer user questions with up-to-date information.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- **Nebius API key** - Sign up for free at [Token Factory](https://tokenfactory.nebius.com/)\n",
        "- **Tavily API key** - Get yours at [app.tavily.com](https://app.tavily.com) (1,000 free API credits/month)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1 - Setup\n",
        "\n",
        "### 1.1 - If running on Google Colab\n",
        "\n",
        "Add `NEBIUS_API_KEY` and `TAVILY_API_KEY` to **Secrets**\n",
        "\n",
        "![](https://github.com/nebius/token-factory-cookbook/raw/main/images/google-colab-1.png)\n",
        "\n",
        "\n",
        "### 1.2 - If running locally\n",
        "\n",
        "Create an `.env` file with both keys:\n",
        "\n",
        "```text\n",
        "NEBIUS_API_KEY=your_nebius_api_key_here\n",
        "TAVILY_API_KEY=tvly-your_tavily_api_key_here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2 - Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NOT running on Colab\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "    print(\"Running on Colab\")\n",
        "    RUNNING_ON_COLAB = True\n",
        "else:\n",
        "    print(\"NOT running on Colab\")\n",
        "    RUNNING_ON_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 26.0.1 is available.\n",
            "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q openai python-dotenv pydantic tavily-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3 - Load Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… NEBIUS_API_KEY found\n",
            "âœ… TAVILY_API_KEY found\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if RUNNING_ON_COLAB:\n",
        "    from google.colab import userdata\n",
        "    NEBIUS_API_KEY = userdata.get('NEBIUS_API_KEY')\n",
        "    TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "else:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    NEBIUS_API_KEY = os.getenv('NEBIUS_API_KEY')\n",
        "    TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')\n",
        "\n",
        "if NEBIUS_API_KEY:\n",
        "    print('\\u2705 NEBIUS_API_KEY found')\n",
        "    os.environ['NEBIUS_API_KEY'] = NEBIUS_API_KEY\n",
        "else:\n",
        "    raise RuntimeError('\\u274c NEBIUS_API_KEY NOT found')\n",
        "\n",
        "if TAVILY_API_KEY:\n",
        "    print('\\u2705 TAVILY_API_KEY found')\n",
        "    os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY\n",
        "else:\n",
        "    raise RuntimeError('\\u274c TAVILY_API_KEY NOT found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4 - Pick a Model\n",
        "\n",
        "We need a model that supports function calling.\n",
        "\n",
        "1. Go to **models** tab in [tokenfactory.nebius.com](https://tokenfactory.nebius.com/)\n",
        "2. Select **text to text** models\n",
        "3. Select a model **function calling** capability\n",
        "4. Copy the model name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL = \"moonshotai/Kimi-K2.5\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5 - Define the Tavily Search Tool\n",
        "\n",
        "We define `tavily_web_search` as a callable tool, then describe its schema using [Pydantic](https://docs.pydantic.dev/) so the LLM knows how to invoke it.\n",
        "\n",
        "The function wraps `TavilyClient.search()` and returns the raw JSON response for the LLM to process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tavily search tool defined\n",
            "\n",
            "Tool schema:\n",
            "[\n",
            "  {\n",
            "    \"type\": \"function\",\n",
            "    \"function\": {\n",
            "      \"name\": \"tavily_web_search\",\n",
            "      \"description\": \"Search the web for current information using Tavily. Use this to answer questions that require up-to-date information, recent events, facts you are uncertain about, or anything beyond your training data.\",\n",
            "      \"parameters\": {\n",
            "        \"properties\": {\n",
            "          \"query\": {\n",
            "            \"description\": \"The search query. Keep under 400 characters. Use a focused search query, not a long prompt.\",\n",
            "            \"title\": \"Query\",\n",
            "            \"type\": \"string\"\n",
            "          },\n",
            "          \"search_depth\": {\n",
            "            \"default\": \"advanced\",\n",
            "            \"description\": \"Search depth: 'basic' for general results, 'advanced' for highest relevance.\",\n",
            "            \"enum\": [\n",
            "              \"basic\",\n",
            "              \"advanced\"\n",
            "            ],\n",
            "            \"title\": \"Search Depth\",\n",
            "            \"type\": \"string\"\n",
            "          },\n",
            "          \"max_results\": {\n",
            "            \"default\": 5,\n",
            "            \"description\": \"Maximum number of search results to return (1-20).\",\n",
            "            \"title\": \"Max Results\",\n",
            "            \"type\": \"integer\"\n",
            "          }\n",
            "        },\n",
            "        \"required\": [\n",
            "          \"query\"\n",
            "        ],\n",
            "        \"title\": \"TavilyWebSearchParams\",\n",
            "        \"type\": \"object\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional, Literal\n",
        "from tavily import TavilyClient\n",
        "\n",
        "# Initialize the Tavily client\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "\n",
        "\n",
        "# --- Pydantic schema for the tool parameters ---\n",
        "class TavilyWebSearchParams(BaseModel):\n",
        "    query: str = Field(\n",
        "        ...,\n",
        "        description=\"The search query. Keep under 400 characters. Use a focused search query, not a long prompt.\"\n",
        "    )\n",
        "    search_depth: Literal[\"basic\", \"advanced\"] = Field(\n",
        "        default=\"advanced\",\n",
        "        description=\"Search depth: 'basic' for general results, 'advanced' for highest relevance.\"\n",
        "    )\n",
        "    max_results: int = Field(\n",
        "        default=5,\n",
        "        description=\"Maximum number of search results to return (1-20).\"\n",
        "    )\n",
        "\n",
        "\n",
        "# --- The actual tool function ---\n",
        "def tavily_web_search(query: str, search_depth: str = \"advanced\",\n",
        "                      topic: str = \"general\", max_results: int = 5,\n",
        "                      time_range: str = None) -> str:\n",
        "    \"\"\"Search the web using Tavily and return raw results.\"\"\"\n",
        "    response = tavily_client.search(\n",
        "        query=query,\n",
        "        search_depth=search_depth,\n",
        "        topic=topic,\n",
        "        max_results=max_results,\n",
        "        time_range=time_range,\n",
        "    )\n",
        "\n",
        "    return json.dumps(response)\n",
        "\n",
        "\n",
        "# --- OpenAI-compatible tool definition ---\n",
        "tools = [{\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"tavily_web_search\",\n",
        "        \"description\": (\n",
        "            \"Search the web for current information using Tavily. \"\n",
        "            \"Use this to answer questions that require up-to-date information, \"\n",
        "            \"recent events, facts you are uncertain about, or anything beyond your training data.\"\n",
        "        ),\n",
        "        \"parameters\": TavilyWebSearchParams.model_json_schema()\n",
        "    }\n",
        "}]\n",
        "\n",
        "available_tools = {\"tavily_web_search\": tavily_web_search}\n",
        "\n",
        "print(\"Tavily search tool defined\")\n",
        "print(f\"\\nTool schema:\\n{json.dumps(tools, indent=2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6 - Initialize the LLM Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI client ready (model: moonshotai/Kimi-K2.5)\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.tokenfactory.nebius.com/v1/\",\n",
        "    api_key=NEBIUS_API_KEY,\n",
        ")\n",
        "\n",
        "print(f\"OpenAI client ready (model: {MODEL})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7 - Tool Calling Loop\n",
        "\n",
        "This is the core pattern: we send a user question to the LLM with the tool definitions, then handle any tool calls the model makes. The loop continues until the model produces a final text response.\n",
        "\n",
        "```\n",
        "User Question\n",
        "     â†“\n",
        "LLM (with tools) â”€â”€â”€â”€â”€â†’ Tool Call? â”€â”€ Yes â”€â”€â†’ Execute tavily_web_search()\n",
        "     â†‘                                              â”‚\n",
        "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Feed results back â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "     â”‚\n",
        "     No â”€â”€â†’ Final Answer\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask(user_question: str, system_prompt: str = None, verbose: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Send a question to the LLM with Tavily search available as a tool.\n",
        "    Handles the full tool-calling loop.\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    messages.append({\"role\": \"user\", \"content\": user_question})\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\U0001f4ac User: {user_question}\\n\")\n",
        "\n",
        "    while True:\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=messages,\n",
        "            tools=tools,\n",
        "        )\n",
        "\n",
        "        choice = response.choices[0]\n",
        "\n",
        "        # If the model wants to call tools\n",
        "        if choice.message.tool_calls:\n",
        "            # Append the assistant message with tool calls\n",
        "            messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"tool_calls\": choice.message.tool_calls\n",
        "            })\n",
        "\n",
        "            for call in choice.message.tool_calls:\n",
        "                tool_fn = available_tools[call.function.name]\n",
        "                args = json.loads(call.function.arguments)\n",
        "\n",
        "                if verbose:\n",
        "                    print(f\"\\U0001f50d Tool call: {call.function.name}\")\n",
        "                    print(f\"   Args: {json.dumps(args, indent=2)}\")\n",
        "\n",
        "                # Execute the tool\n",
        "                result = tool_fn(**args)\n",
        "\n",
        "                if verbose:\n",
        "                    # Show a preview of the results\n",
        "                    preview = result[:300] + \"...\" if len(result) > 300 else result\n",
        "                    print(f\"   Result preview: {preview}\\n\")\n",
        "\n",
        "                # Feed the tool result back to the LLM\n",
        "                messages.append({\n",
        "                    \"role\": \"tool\",\n",
        "                    \"content\": result,\n",
        "                    \"tool_call_id\": call.id,\n",
        "                    \"name\": call.function.name\n",
        "                })\n",
        "        else:\n",
        "            # No tool calls - we have the final answer\n",
        "            answer = choice.message.content\n",
        "            if verbose:\n",
        "                print(f\"\\U0001f916 Assistant:\\n{answer}\")\n",
        "            return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8 - Try It Out\n",
        "\n",
        "### Example 1: Current events\n",
        "\n",
        "Ask a question that requires recent information the LLM wouldn't have in its training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¬ User: What are the latest developments in AI in February 2026?\n",
            "\n",
            "ðŸ” Tool call: tavily_web_search\n",
            "   Args: {\n",
            "  \"query\": \"AI artificial intelligence latest developments February 2026\",\n",
            "  \"max_results\": 10,\n",
            "  \"search_depth\": \"advanced\"\n",
            "}\n",
            "   Result preview: {\"query\": \"AI artificial intelligence latest developments February 2026\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.marketingprofs.com/opinions/2026/54304/ai-update-february-13-2026-ai-news-and-views-from-the-past-week\", \"title\": \"AI Update, February...\n",
            "\n",
            "ðŸ¤– Assistant:\n",
            " Based on the latest search results, here are the major AI developments taking place in February 2026:\n",
            "\n",
            "## ðŸš€ Major Model Releases (Early February)\n",
            "\n",
            "**February 5-7 saw significant model launches:**\n",
            "- **Anthropic Claude Opus 4.6** (Feb 5): Features a 1 million-token context window, enhanced task planning, and multi-agent collaboration for parallel workflows\n",
            "- **OpenAI GPT-5.3-Codex** (Feb 7): Focused on coding capabilities\n",
            "- **China's Zhipu GLM-5**: Topped open-source benchmarks, representing China's growing competitiveness in AI\n",
            "\n",
            "## ðŸ¤– Agentic AI Goes Mainstream\n",
            "\n",
            "**The Model Context Protocol (MCP)** has gained widespread adoption as an industry standard, with Anthropic donating it to the Linux Foundation and adoption by OpenAI, Microsoft, and Google.\n",
            "\n",
            "**OpenAI's Responses API Upgrades:**\n",
            "- Support for agent skills and SKILL.md manifests\n",
            "- Server-side compaction for long-running tasks without context loss\n",
            "- Hosted shell containers in managed Debian 12 environments\n",
            "- Persistent storage and networking capabilities\n",
            "\n",
            "## ðŸ”„ \"Vibe Coding\" Revolution\n",
            "\n",
            "Spotify announced their senior engineers **haven't written a single line of code since December 2025**, shifting entirely to AI-generated code supervision. This \"vibe coding\" approachâ€”where developers describe intent rather than writing codeâ€”now powers 95% of AI-generated codebases.\n",
            "\n",
            "## ðŸ“Š Infrastructure Reality Check\n",
            "\n",
            "The industry faces challenges with massive AI data centers causing environmental and resource concerns. Meanwhile, **Cerebras** (AI chip company) is reportedly exploring a 2027 IPO with its valuation nearly tripling in six months, signaling strong investor confidence in alternative compute architectures.\n",
            "\n",
            "## âš ï¸ AI Safety Concerns Escalating\n",
            "\n",
            "Researchers from leading AI firms (OpenAI, Anthropic) are publicly warning about escalating risks as models become more autonomous. Internal reports highlight concerns about:\n",
            "- AI-enabled crime\n",
            "- Self-directed compute acquisition\n",
            "- Models contributing to their own development with limited oversight\n",
            "\n",
            "## ðŸŒ Regulatory Landscape\n",
            "\n",
            "- The **2026 International AI Safety Report** was published (Feb 3)\n",
            "- In the US, a December 2025 executive order signaled intent to limit conflicting state AI laws in favor of national policy\n",
            "- India hosted the AI Impact Summit, positioning itself as a Global South leader in ethical AI governance\n",
            "\n",
            "## ðŸ“ˆ Key Trends\n",
            "\n",
            "1. **Shift from scale to efficiency**: The industry is moving away from simply building larger models toward smarter, more efficient architectures\n",
            "2. **Recursive Language Models (RLMs)**: New architectures handling 10M+ tokens through recursive prompt processing rather than expanding context windows\n",
            "3. **Open-source momentum**: About 80% of startups are now building on Chinese open-source models like Qwen and DeepSeek due to lower costs\n",
            "\n",
            "The consensus among industry observers is that 2026 marks the transition from AI as tools to AI as autonomous workers, with widespread white-collar disruption potentially arriving within 1-5 years.\n",
            "CPU times: user 34.4 ms, sys: 6.66 ms, total: 41 ms\n",
            "Wall time: 33.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "answer = ask(\"What are the latest developments in AI in February 2026?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Factual lookup\n",
        "\n",
        "A question where the model should search to verify up-to-date facts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¬ User: Who won the 2026 Super Bowl and what was the final score and who prefromed at halftime?\n",
            "\n",
            "ðŸ” Tool call: tavily_web_search\n",
            "   Args: {\n",
            "  \"query\": \"2026 Super Bowl winner final score halftime performer\",\n",
            "  \"max_results\": 5,\n",
            "  \"search_depth\": \"advanced\"\n",
            "}\n",
            "   Result preview: {\"query\": \"2026 Super Bowl winner final score halftime performer\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.foxsports.com/stories/nfl/2026-super-bowl-halftime-show-who-performing-time-more\", \"title\": \"2026 Super Bowl Halftime Show: Bad Bunny Perform...\n",
            "\n",
            "ðŸ¤– Assistant:\n",
            " Based on the search results, here are the details for the **2026 Super Bowl (Super Bowl LX)**:\n",
            "\n",
            "**Winner:** The **Seattle Seahawks** defeated the New England Patriots\n",
            "\n",
            "**Final Score:** **Seattle Seahawks 29, New England Patriots 13**\n",
            "\n",
            "**Halftime Performer:** **Bad Bunny** headlined the halftime show, making history as one of the youngest performers to do so. He was joined on stage by **Lady Gaga** and **Ricky Martin**.\n",
            "\n",
            "**Additional Notes:**\n",
            "- The game was held at Levi's Stadium in Santa Clara, California (February 8, 2026)\n",
            "- This was a Super Bowl XLIX rematch, where the Patriots had previously beaten the Seahawks 28-24\n",
            "- **Kenneth Walker III** (Seahawks running back) was named Super Bowl MVP\n",
            "CPU times: user 29.6 ms, sys: 5.04 ms, total: 34.6 ms\n",
            "Wall time: 9.55 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "answer = ask(\"Who won the 2026 Super Bowl and what was the final score and who prefromed at halftime?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Technical research\n",
        "\n",
        "A developer question that benefits from current web search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¬ User: What are the key differences between Llama 4 and Qwen3?\n",
            "\n",
            "ðŸ” Tool call: tavily_web_search\n",
            "   Args: {\n",
            "  \"query\": \"Llama 4 vs Qwen3 comparison key differences 2024 2025\",\n",
            "  \"max_results\": 7,\n",
            "  \"search_depth\": \"advanced\"\n",
            "}\n",
            "   Result preview: {\"query\": \"Llama 4 vs Qwen3 comparison key differences 2024 2025\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison\", \"title\": \"The Big LLM Architecture Comparison - Ahead of AI\", \"content\": \"L...\n",
            "\n",
            "ðŸ” Tool call: tavily_web_search\n",
            "   Args: {\n",
            "  \"query\": \"Meta Llama 4 features architecture availability 2024 2025\",\n",
            "  \"max_results\": 7,\n",
            "  \"search_depth\": \"advanced\"\n",
            "}\n",
            "   Result preview: {\"query\": \"Meta Llama 4 features architecture availability 2024 2025\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://gpt-trainer.com/blog/llama+4+evolution+features+comparison\", \"title\": \"Llama 4: Meta's New AI Model - Evolution, Features, and Comparison\", \"...\n",
            "\n",
            "ðŸ” Tool call: tavily_web_search\n",
            "   Args: {\n",
            "  \"query\": \"Alibaba Qwen3 model features architecture availability Apache license 2025\",\n",
            "  \"max_results\": 7,\n",
            "  \"search_depth\": \"advanced\"\n",
            "}\n",
            "   Result preview: {\"query\": \"Alibaba Qwen3 model features architecture availability Apache license 2025\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://en.wikipedia.org/wiki/Qwen\", \"title\": \"Qwen - Wikipedia\", \"content\": \"On September 5, 2025, Alibaba launched Qwen3-Max._[cit...\n",
            "\n",
            "ðŸ¤– Assistant:\n",
            " Based on my research, here are the key differences between **Llama 4** (Meta) and **Qwen3** (Alibaba Cloud):\n",
            "\n",
            "## 1. Architecture Approach\n",
            "\n",
            "| Feature | Llama 4 | Qwen3 |\n",
            "|---------|---------|-------|\n",
            "| **Primary Architecture** | Mixture-of-Experts (MoE) only | Mixture-of-Experts + Dense variants |\n",
            "| **Model Variants** | Scout (109B total, 17B active), Maverick (400B total, 17B active), Behemoth (2T total) | Dense: 0.6B to 32B; MoE: 30B-A3B, 235B-A22B |\n",
            "| **Reasoning System** | Standard instruction-tuned | **Hybrid thinking modes** with reasoning on/off toggle |\n",
            "| **Attention** | GQA (Grouped Query Attention) | GQA with variations in head dimensions |\n",
            "\n",
            "**Insight:** Qwen3 offers more flexibility by providing both dense and MoE options, including very small models (0.6B) suitable for edge devices, while Llama 4 commits fully to MoE with fewer size options [Source](https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison).\n",
            "\n",
            "## 2. Multimodality\n",
            "\n",
            "| Feature | Llama 4 | Qwen3 |\n",
            "|---------|---------|-------|\n",
            "| **Native Multimodal** | **Yes** â€“ text, images, video via early fusion | Initially text-only; multimodal via separate models (Qwen3-Omni) |\n",
            "| **Vision Capabilities** | Built-in for Scout/Maverick | Via Qwen3-Omni (separate release) |\n",
            "\n",
            "**Insight:** Llama 4 is natively multimodal at launch for all variants. While Qwen3 initially focused on text, they later released Qwen3-Omni for multimodal capabilities [Source](https://en.wikipedia.org/wiki/Qwen), [Source](https://www.opensourceforu.com/2025/09/alibaba-qwen-team-launches-qwen3-omni-as-fully-open-source-multimodal-ai-model/).\n",
            "\n",
            "## 3. Context Window\n",
            "\n",
            "| Model | Context Window |\n",
            "|-------|----------------|\n",
            "| **Llama 4 Scout** | **10 million tokens** (~7,500 pages) |\n",
            "| **Llama 4 Maverick** | 1 million tokens (expandable to 10M) |\n",
            "| **Qwen3** | 32Kâ€“128K tokens (depending on model size) |\n",
            "\n",
            "**Insight:** Llama 4 Scout is specifically designed for extremely long contexts (10M tokens), making it ideal for multi-document analysis and massive codebases, while Qwen3 offers much more limited context windows [Source](https://llm-stats.com/models/compare/llama-4-scout-vs-qwen3-32b), [Source](https://introl.com/blog/open-source-ai-models-december-2025).\n",
            "\n",
            "## 4. Licensing & Usage\n",
            "\n",
            "| Feature | Llama 4 | Qwen3 |\n",
            "|---------|---------|-------|\n",
            "| **License** | **Llama 4 Community License** (custom) | **Apache 2.0** |\n",
            "| **Commercial Restrictions** | Companies with >700M users must request separate license | None â€“ fully permissive |\n",
            "| **Model Weights** | Open weights | Open weights |\n",
            "\n",
            "**Insight:** Qwen3's Apache 2.0 license is significantly more permissive for commercial use. Llama 4 has restrictions that could prevent large companies from using it freely without Meta's approval [Source](https://llm-stats.com/models/compare/llama-4-scout-vs-qwen3-32b), [Source](https://www.interconnects.ai/p/qwen-3-the-new-open-standard).\n",
            "\n",
            "## 5. Performance on Benchmarks\n",
            "\n",
            "Based on head-to-head comparisons between Llama 4 Scout and Qwen3 models:\n",
            "\n",
            "| Benchmark | Llama 4 Scout | Qwen3 32B (Dense) | Winner |\n",
            "|-----------|---------------|-------------------|--------|\n",
            "| **LiveCodeBench** | 32.8% | 65.7% | Qwen3 |\n",
            "| **GPQA** | 57.2% | â€” | â€” |\n",
            "| **Latency** | 0.31 ms | 1.19 ms | Llama 4 |\n",
            "| **Throughput** | 76.1 tokens/s | 26.95 tokens/s | Llama 4 |\n",
            "\n",
            "Comparing Llama 4 Scout vs Qwen3 30B-A3B (MoE):\n",
            "- Qwen3 dominates in coding (LiveCodeBench: 62.6% vs 32.8%)\n",
            "- Qwen3 leads in GPQA (65.8% vs 57.2%)\n",
            "\n",
            "**Insight:** Despite Llama 4's larger parameter count, Qwen3 models typically outperform on reasoning and coding benchmarks. However, Llama 4 offers faster latency and throughput [Source](https://llm-stats.com/models/compare/llama-4-scout-vs-qwen3-32b).\n",
            "\n",
            "## 6. Reasoning Capabilities\n",
            "\n",
            "**Qwen3 introduces \"hybrid thinking modes\":**\n",
            "- **Non-thinking mode**: Fast responses for simple queries\n",
            "- **Thinking mode**: Step-by-step reasoning for complex problems\n",
            "- Toggle between modes during conversation\n",
            "\n",
            "**Llama 4**: Standard instruction tuning without explicit reasoning mode switching [Source](https://qwenlm.github.io/blog/qwen3/).\n",
            "\n",
            "## 7. Language Support & Training\n",
            "\n",
            "| Feature | Llama 4 | Qwen3 |\n",
            "|---------|---------|-------|\n",
            "| **Training Tokens** | Scout: 40T; Maverick: 22T | 36T |\n",
            "| **Languages** | 12 languages (Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, Vietnamese) | 119 languages and dialects |\n",
            "| **Knowledge Cutoff** | August 2024 | Not explicitly stated |\n",
            "\n",
            "**Insight:** Qwen3 has broader multilingual support from the start, while Llama 4 focuses on fewer major languages [Source](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Original), [Source](https://en.wikipedia.org/wiki/Qwen).\n",
            "\n",
            "## Summary Table\n",
            "\n",
            "| Factor | Llama 4 | Qwen3 |\n",
            "|--------|---------|-------|\n",
            "| **Best For** | Long-context tasks, multimodal applications, fast inference | Coding, reasoning, commercial deployment, language diversity |\n",
            "| **Model Variety** | Fewer but larger models | Very wide range (0.6Bâ€“235B) |\n",
            "| **License Flexibility** | Restrictions for large companies | Fully open (Apache 2.0) |\n",
            "| **Benchmark Leadership** | Better efficiency metrics | Superior on coding/reasoning tasks |\n",
            "| **Release Philosophy** | Big splash releases | Rapid iteration, many variants |\n",
            "\n",
            "Both represent cutting-edge open-weight AI, but your choice depends on use case: Llama 4 excels for long-document processing and multimodal needs, while Qwen3 offers superior coding performance, more permissive licensing, and greater size flexibility.\n",
            "CPU times: user 71.7 ms, sys: 20.4 ms, total: 92.1 ms\n",
            "Wall time: 58.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "answer = ask(\n",
        "    \"What are the key differences between Llama 4 and Qwen3?\",\n",
        "    system_prompt=\"You are a helpful AI assistant with access to web search. Cite your sources with URLs.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 4: No search needed\n",
        "\n",
        "For questions the model can answer from training data, it should skip the tool call entirely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¬ User: What is the capital of France?\n",
            "\n",
            "ðŸ¤– Assistant:\n",
            " The capital of France is **Paris**.\n",
            "CPU times: user 9.74 ms, sys: 8.76 ms, total: 18.5 ms\n",
            "Wall time: 2.74 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "answer = ask(\"What is the capital of France?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9 - Try Your Own Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "answer = ask(\"YOUR QUESTION HERE\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
